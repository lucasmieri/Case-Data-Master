{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "1.Read and Analyze Data\n",
      "\n",
      "\n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "Reading file: c:\\Users\\Lucas\\OneDrive\\case data master\\data\\raw\\members.parquet\n",
      "\n",
      "\n",
      "--- Analyzing members.parquet DataFrame ---\n",
      "\n",
      "\n",
      "Displaying data sample 10 rows:\n",
      "\n",
      "\n",
      "\n",
      "| msno                                         |   safra |   registration_init_time |   city |   bd | gender   |   registered_via |   is_ativo |\n",
      "|:---------------------------------------------|--------:|-------------------------:|-------:|-----:|:---------|-----------------:|-----------:|\n",
      "| +++snpr7pmobhLKUgSHTv/mpkqgBT0tQJ0zQj6qKrqc= |  201612 |                 20140927 |      1 |    0 | <NA>     |                7 |          1 |\n",
      "| ++/AwGzubug3gT6J+0STBGMdWKxaM+UFZTI8Tcmq4To= |  201607 |                 20150322 |      1 |    0 | <NA>     |                9 |          0 |\n",
      "| ++/Gw1B9K+XOlB3hLTloeUK2QlCa2m+BJ8TrzGf7djI= |  201601 |                 20121217 |     15 |   32 | male     |                3 |          1 |\n",
      "| ++02XbtviomSxcIBUHMOiJkjRxdicTXSfiVqLdsr5lo= |  201603 |                 20131112 |     14 |   21 | male     |                7 |          0 |\n",
      "| ++0O0Bq04sB/9ZcOS+pajpYL2Hin9jCqnc/8bKzKFuE= |  201610 |                 20141021 |      5 |   33 | male     |                3 |          0 |\n",
      "| ++2AQgVgYUAqJDw684tbDqDffUeKhqydyQmbr8lz9lQ= |  201608 |                 20150416 |     18 |   23 | male     |                3 |          0 |\n",
      "| ++2gRJ7i2MbO6qUG6rGfFnu/Fcv+hdX4YvTkZD+PUsk= |  201608 |                 20140616 |     11 |   33 | male     |                7 |          0 |\n",
      "| ++3brN43Yd6GURegTBR85oMQcJrgW1+/N4B8Rjj75fY= |  201604 |                 20100512 |     14 |   19 | male     |                9 |          0 |\n",
      "| ++4FwgRp7pHuuQWpaUFrCTgJbXVwNTTQpLB2bM1A3lU= |  201605 |                 20141010 |      1 |    0 | <NA>     |                9 |          0 |\n",
      "| ++4KsBMCDgIrwmv5w2g2cCXhzCsJwDyB5r96/FrwGek= |  201604 |                 20141031 |      1 |    0 | <NA>     |                9 |          0 |\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "DataFrame info for members.parquet:\n",
      "\n",
      "\n",
      "msno                      string[pyarrow]\n",
      "safra                     string[pyarrow]\n",
      "registration_init_time    string[pyarrow]\n",
      "city                      string[pyarrow]\n",
      "bd                        string[pyarrow]\n",
      "gender                    string[pyarrow]\n",
      "registered_via            string[pyarrow]\n",
      "is_ativo                            int32\n",
      "dtype: object\n",
      "\n",
      "\n",
      "\n",
      "Summary statistics:\n",
      "\n",
      "\n",
      "           is_ativo\n",
      "count  6.386725e+07\n",
      "mean   1.760349e-01\n",
      "std    3.808499e-01\n",
      "min    0.000000e+00\n",
      "25%    0.000000e+00\n",
      "50%    0.000000e+00\n",
      "75%    0.000000e+00\n",
      "max    1.000000e+00\n",
      "\n",
      "\n",
      "\n",
      "Missing values per column:\n",
      "\n",
      "\n",
      "msno                             0\n",
      "safra                            0\n",
      "registration_init_time           0\n",
      "city                             0\n",
      "bd                               0\n",
      "gender                    38210177\n",
      "registered_via                   0\n",
      "is_ativo                         0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "Reading file: c:\\Users\\Lucas\\OneDrive\\case data master\\data\\raw\\transactions.parquet\n",
      "\n",
      "\n",
      "--- Analyzing transactions.parquet DataFrame ---\n",
      "\n",
      "\n",
      "Displaying data sample 10 rows:\n",
      "\n",
      "\n",
      "\n",
      "| msno                                         |   payment_method_id |   payment_plan_days |   plan_list_price |   actual_amount_paid |   is_auto_renew |   transaction_date |   membership_expire_date |   is_cancel |   safra |\n",
      "|:---------------------------------------------|--------------------:|--------------------:|------------------:|---------------------:|----------------:|-------------------:|-------------------------:|------------:|--------:|\n",
      "| +++IZseRRiQS9aaSkH6cMYU6bGDcxUieAi/tH67sC5s= |                  38 |                 410 |              1788 |                 1788 |               0 |           20151121 |                 20170104 |           0 |  201511 |\n",
      "| +++snpr7pmobhLKUgSHTv/mpkqgBT0tQJ0zQj6qKrqc= |                  41 |                  30 |               149 |                  149 |               1 |           20150526 |                 20150626 |           0 |  201505 |\n",
      "| +++snpr7pmobhLKUgSHTv/mpkqgBT0tQJ0zQj6qKrqc= |                  41 |                  30 |               149 |                  149 |               1 |           20150926 |                 20151026 |           0 |  201509 |\n",
      "| ++/9R3sX37CjxbY/AaGvbwr3QkwElKBCtSvVzhCBDOk= |                  41 |                  30 |               149 |                  149 |               1 |           20160615 |                 20160715 |           0 |  201606 |\n",
      "| ++/Gw1B9K+XOlB3hLTloeUK2QlCa2m+BJ8TrzGf7djI= |                  40 |                  31 |               149 |                  149 |               1 |           20150113 |                 20150216 |           0 |  201501 |\n",
      "| ++0nOC7BmrUTtcSboRORfg6ZXTajnBDt1f/SEgH6ONo= |                  16 |                  30 |               149 |                  149 |               1 |           20151012 |                 20151111 |           0 |  201510 |\n",
      "| ++0t0Uy2D3r1pRVxg28G3r3l5PfhFlCPMGElwHqbYL8= |                  35 |                   7 |                 0 |                    0 |               0 |           20150614 |                 20150615 |           0 |  201506 |\n",
      "| ++1eOqCPRmzyBjMGvAJEaurjI1AFz4Mify6fk2eecbY= |                  35 |                   7 |                 0 |                    0 |               0 |           20161101 |                 20161108 |           0 |  201611 |\n",
      "| ++2wdWRV3Thy9HZyRJtKxlNsa55oDiDc7arR1guiypc= |                  41 |                  30 |               149 |                    0 |               1 |           20151008 |                 20151015 |           1 |  201510 |\n",
      "| ++3A6JMzYJeron30GTcDostfXoAl8rTBuB2M8GeVdNU= |                  41 |                  30 |               149 |                  119 |               1 |           20150611 |                 20150711 |           0 |  201506 |\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "DataFrame info for transactions.parquet:\n",
      "\n",
      "\n",
      "msno                      string[pyarrow]\n",
      "payment_method_id         string[pyarrow]\n",
      "payment_plan_days         string[pyarrow]\n",
      "plan_list_price           string[pyarrow]\n",
      "actual_amount_paid        string[pyarrow]\n",
      "is_auto_renew             string[pyarrow]\n",
      "transaction_date          string[pyarrow]\n",
      "membership_expire_date    string[pyarrow]\n",
      "is_cancel                 string[pyarrow]\n",
      "safra                               int32\n",
      "dtype: object\n",
      "\n",
      "\n",
      "\n",
      "Summary statistics:\n",
      "\n",
      "\n",
      "              safra\n",
      "count  2.071222e+07\n",
      "mean   2.015764e+05\n",
      "std    6.155761e+01\n",
      "min    2.015010e+05\n",
      "25%    2.015090e+05\n",
      "50%    2.016030e+05\n",
      "75%    2.016090e+05\n",
      "max    2.017020e+05\n",
      "\n",
      "\n",
      "\n",
      "Missing values per column:\n",
      "\n",
      "\n",
      "msno                      0\n",
      "payment_method_id         0\n",
      "payment_plan_days         0\n",
      "plan_list_price           0\n",
      "actual_amount_paid        0\n",
      "is_auto_renew             0\n",
      "transaction_date          0\n",
      "membership_expire_date    0\n",
      "is_cancel                 0\n",
      "safra                     0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "Reading file: c:\\Users\\Lucas\\OneDrive\\case data master\\data\\raw\\user_logs.parquet\n",
      "\n",
      "\n",
      "--- Analyzing user_logs.parquet DataFrame ---\n",
      "\n",
      "\n",
      "Displaying data sample 10 rows:\n",
      "\n",
      "\n",
      "\n",
      "| msno                                         |   safra |   num_25 |   num_50 |   num_75 |   num_985 |   num_100 |   num_unq |   total_secs |\n",
      "|:---------------------------------------------|--------:|---------:|---------:|---------:|----------:|----------:|----------:|-------------:|\n",
      "| SwlrSivYHoKF9V5wm1YYYAnjHpd9y3OPjI9rDUhGJ3k= |  201701 |      121 |       28 |       14 |        29 |       704 |       827 |     184607   |\n",
      "| rE5wSmHEF1Dhu55zhkiGB1HvotdlSHcIMGXv6VcqO2A= |  201605 |       26 |        2 |        5 |         6 |       462 |       256 |     119439   |\n",
      "| hx+cyaQ/Jcdr/Z5foa/Cn0PXUzC/F7QO/NQvWQS1Qtc= |  201611 |      161 |       71 |       49 |        34 |       668 |       891 |     204791   |\n",
      "| 53QW6B70J23X2UCvxaaUppjyE0b6X9nzP79W4huZv+Q= |  201502 |       37 |        9 |        3 |         9 |       408 |       447 |     101186   |\n",
      "| /0S1N/oRyxGLZlzxnW5rOjfo0ZAls9EH23ahuDNuqz8= |  201506 |      205 |       49 |       23 |        21 |       225 |       489 |      69957.5 |\n",
      "| qB/zteXKaOk3hzFCoIUD6wrTp57hnreDX4Vvon25MfM= |  201509 |       52 |       10 |       10 |        20 |       308 |       264 |      81703.9 |\n",
      "| 7btpXOqzA1ggOggSW81L05zDYyDjO7dXgmwZVzYmI2Q= |  201610 |      159 |       53 |       30 |        43 |      1075 |       881 |     305354   |\n",
      "| kgEhriAqTydVKQ1xn+ZzKQzf4sQ1aod5zcEg5ksyWrE= |  201609 |       88 |       13 |       14 |        13 |       588 |       325 |     159512   |\n",
      "| 8uQ6M7OzdWsuzo0BRZ6siIPZfBoG43bRvlm+My36B6k= |  201509 |       62 |       27 |       15 |         6 |      1389 |      1427 |     324691   |\n",
      "| T2gUhlBhFMoSFA9jFI/BkuyE/EPA6oneabYQiGBy9wU= |  201702 |       36 |       23 |        5 |         9 |       216 |       241 |      56296.3 |\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "DataFrame info for user_logs.parquet:\n",
      "\n",
      "\n",
      "msno          string[pyarrow]\n",
      "safra                   int32\n",
      "num_25                float64\n",
      "num_50                float64\n",
      "num_75                float64\n",
      "num_985               float64\n",
      "num_100               float64\n",
      "num_unq               float64\n",
      "total_secs            float64\n",
      "dtype: object\n",
      "\n",
      "\n",
      "\n",
      "Summary statistics:\n",
      "\n",
      "\n",
      "              safra        num_25        num_50        num_75       num_985  \\\n",
      "count  2.675897e+07  2.675897e+07  2.675897e+07  2.675897e+07  2.675897e+07   \n",
      "mean   2.015720e+05  9.542601e+01  2.403567e+01  1.490352e+01  1.653867e+01   \n",
      "std    6.187618e+01  1.752883e+02  3.912555e+01  2.262672e+01  3.740916e+01   \n",
      "min    2.015010e+05  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "25%    2.015080e+05  1.000000e+01  3.000000e+00  2.000000e+00  2.000000e+00   \n",
      "50%    2.016020e+05  4.100000e+01  1.100000e+01  7.000000e+00  7.000000e+00   \n",
      "75%    2.016090e+05  1.150000e+02  3.100000e+01  2.000000e+01  2.100000e+01   \n",
      "max    2.017020e+05  1.118640e+05  8.875000e+03  3.485000e+03  3.769800e+04   \n",
      "\n",
      "            num_100       num_unq    total_secs  \n",
      "count  2.675897e+07  2.675897e+07  2.675897e+07  \n",
      "mean   4.501598e+02  4.409193e+02 -2.117175e+13  \n",
      "std    7.253139e+02  5.906060e+02  6.839044e+14  \n",
      "min    0.000000e+00  1.000000e+00 -2.398077e+17  \n",
      "25%    3.800000e+01  5.200000e+01  1.143277e+04  \n",
      "50%    2.160000e+02  2.440000e+02  5.970589e+04  \n",
      "75%    5.710000e+02  6.080000e+02  1.541670e+05  \n",
      "max    1.967410e+05  3.270600e+04  9.223372e+15  \n",
      "\n",
      "\n",
      "\n",
      "Missing values per column:\n",
      "\n",
      "\n",
      "msno          0\n",
      "safra         0\n",
      "num_25        0\n",
      "num_50        0\n",
      "num_75        0\n",
      "num_985       0\n",
      "num_100       0\n",
      "num_unq       0\n",
      "total_secs    0\n",
      "dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import dask.dataframe as dd\n",
    "\n",
    "#log.py\n",
    "def configure_logging(log_dir: str = 'log', log_level: str = 'INFO', log_format: str = None) -> logging.Logger:\n",
    "    \"\"\"\n",
    "    Configures logging settings for the application and returns a logger instance.\n",
    "\n",
    "    Parameters:\n",
    "    log_dir (str): The directory where the log file will be saved.\n",
    "    log_level (str): The logging level (INFO, DEBUG, WARNING, ERROR).\n",
    "    log_format (str): The logging format string.\n",
    "\n",
    "    Returns:\n",
    "    logging.Logger: A logger instance for logging messages.\n",
    "    \"\"\"\n",
    "    if log_format is None:\n",
    "        log_format = '%(asctime)s - %(levelname)s - %(message)s'+'\\n'\n",
    "    \n",
    "    log_level = log_level.upper()\n",
    "    log_levels = {'DEBUG': logging.DEBUG, 'INFO': logging.INFO, 'WARNING': logging.WARNING, 'ERROR': logging.ERROR}\n",
    "    level = log_levels.get(log_level, logging.INFO)\n",
    "\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "\n",
    "    log_file_name = f'log_src_{datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}.log'\n",
    "    log_file_path = os.path.join(log_dir, log_file_name)\n",
    "\n",
    "    logging.basicConfig(filename=log_file_path, level=level, format=log_format, filemode='w')\n",
    "    \n",
    "    logger = logging.getLogger()\n",
    "    return logger\n",
    "\n",
    "def log_message(logger: logging.Logger, level: str, message: str) -> None:\n",
    "    \"\"\"\n",
    "    Logs a message at the specified level using the given logger.\n",
    "\n",
    "    Parameters:\n",
    "    logger (logging.Logger): The logger instance.\n",
    "    level (str): The level of the log ('INFO', 'ERROR', 'WARNING', etc.).\n",
    "    message (str): The log message.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    if level.upper() == 'INFO':\n",
    "        logger.info(message)\n",
    "    elif level.upper() == 'WARNING':\n",
    "        logger.warning(message)\n",
    "    elif level.upper() == 'ERROR':\n",
    "        logger.error(message)\n",
    "    elif level.upper() == 'DEBUG':\n",
    "        logger.debug(message)\n",
    "    else:\n",
    "        logger.info(message)\n",
    "\n",
    "# prints.py\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "def custom_print(logger: logging.Logger, message: str, skip_lines: bool = True, space_lines:bool=False) -> None:\n",
    "    \"\"\"\n",
    "    Custom print function to print to both the console and the log file.\n",
    "\n",
    "    Parameters:\n",
    "    logger (logging.Logger): The logger instance.\n",
    "    message (str): The message to print.\n",
    "    skip_lines (bool): If True, adds an extra blank line for readability.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    if space_lines:\n",
    "        space='_'*150+\"\\n\"\n",
    "    else:\n",
    "        space=''\n",
    "    if skip_lines:\n",
    "        message = \"\\n\" +space+ message + \"\\n\"\n",
    "\n",
    "    print(message)\n",
    "    logger.info(message.strip())\n",
    "    \n",
    "def print_error(logger: logging.Logger, message: str) -> None:\n",
    "    \"\"\"\n",
    "    Custom print function to handle error messages.\n",
    "\n",
    "    Parameters:\n",
    "    logger (logging.Logger): The logger instance.\n",
    "    message (str): The error message to print.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    error_message = f\"ERROR: {message}\"\n",
    "    print(error_message)\n",
    "    logger.error(error_message)\n",
    "\n",
    "def print_warning(logger: logging.Logger, message: str) -> None:\n",
    "    \"\"\"\n",
    "    Custom print function to handle warning messages.\n",
    "\n",
    "    Parameters:\n",
    "    logger (logging.Logger): The logger instance.\n",
    "    message (str): The warning message to print.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    warning_message = f\"WARNING: {message}\"\n",
    "    print(warning_message)\n",
    "    logger.warning(warning_message)\n",
    "\n",
    "def print_success(logger: logging.Logger, message: str) -> None:\n",
    "    \"\"\"\n",
    "    Custom print function to handle success messages.\n",
    "\n",
    "    Parameters:\n",
    "    logger (logging.Logger): The logger instance.\n",
    "    message (str): The success message to print.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    success_message = f\"SUCCESS: {message}\"\n",
    "    print(success_message)\n",
    "    logger.info(success_message)\n",
    "\n",
    "\n",
    "def print_info(logger: logging.Logger, message: str, additional_info: str = None) -> None:\n",
    "    \"\"\"\n",
    "    Custom print function to handle informational messages.\n",
    "\n",
    "    Parameters:\n",
    "    logger (logging.Logger): The logger instance.\n",
    "    message (str): The informational message to print.\n",
    "    additional_info (str): Additional info to print, if provided.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    info_message = f\"INFO: {message}\"\n",
    "    if additional_info:\n",
    "        info_message += f\" | Additional Info: {additional_info}\"\n",
    "\n",
    "    print(info_message)\n",
    "    logger.info(info_message)\n",
    "\n",
    "\n",
    "def register_time_spent(logger: logging.Logger, start_time: datetime, process_name: str) -> None:\n",
    "    \"\"\"\n",
    "    Logs the time spent on a process.\n",
    "\n",
    "    Parameters:\n",
    "    logger (logging.Logger): The logger instance.\n",
    "    start_time (datetime): The start time of the process.\n",
    "    process_name (str): A string indicating the name of the process.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    end_time = datetime.now()\n",
    "    time_spent = (end_time - start_time).total_seconds()  \n",
    "    message = f\"Time spent on {process_name}: {time_spent:.2f} seconds\"  \n",
    "    logger.info(message)\n",
    "    \n",
    "#read_data.py\n",
    "def read_parquet_file(file_path: str, logger: logging.Logger) -> dd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads a Parquet file into a Dask DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    file_path (str): The full path to the Parquet file.\n",
    "    logger (logging.Logger): The logger instance for logging operations.\n",
    "\n",
    "    Returns:\n",
    "    dd.DataFrame: The loaded Dask DataFrame or None if the read fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        custom_print(logger, f\"Reading file: {file_path}\", space_lines=True)\n",
    "        start_time = datetime.now()\n",
    "\n",
    "        dask_df = dd.read_parquet(file_path, split_row_groups=True)\n",
    "\n",
    "        register_time_spent(logger, start_time, f\"Reading {file_path}\")\n",
    "\n",
    "        return dask_df\n",
    "    except Exception as e:\n",
    "        print_error(logger, f\"Failed to read file: {file_path}. Error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def analyze_dask_dataframe(dask_df: dd.DataFrame, name: str, logger: logging.Logger, n: int = 10) -> None:\n",
    "    \"\"\"\n",
    "    Analyzes a Dask DataFrame and logs various metrics like info and summary statistics.\n",
    "\n",
    "    Parameters:\n",
    "    dask_df (dd.DataFrame): The Dask DataFrame to analyze.\n",
    "    name (str): The name of the DataFrame for logging.\n",
    "    logger (logging.Logger): The logger instance.\n",
    "    n (int): The number of rows to print during the analysis.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    start_time = datetime.now()\n",
    "    custom_print(logger, f\"--- Analyzing {name} DataFrame ---\")\n",
    "\n",
    "    try:\n",
    "        custom_print(logger, f\"Displaying data sample {n} rows:\")\n",
    "        df_sample = dask_df.head(n)\n",
    "        df_markdown = df_sample.to_markdown(index=False)\n",
    "        custom_print(logger, f\"\\n{df_markdown}\\n\", skip_lines=True)\n",
    "        \n",
    "        custom_print(logger, f\"\\nDataFrame info for {name}:\")\n",
    "        custom_print(logger, str(dask_df.dtypes))\n",
    "        \n",
    "        custom_print(logger, \"\\nSummary statistics:\")\n",
    "        custom_print(logger, str(dask_df.describe().compute()))\n",
    "        \n",
    "        custom_print(logger, \"\\nMissing values per column:\")\n",
    "        custom_print(logger, str(dask_df.isnull().sum().compute()))\n",
    "        \n",
    "        register_time_spent(logger, start_time, f\"Analyzing {name} DataFrame\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print_error(logger, f\"Error analyzing {name}: {str(e)}\")\n",
    "\n",
    "\n",
    "def read_and_analyze_files(data_dir: str, file_names: list, logger: logging.Logger) -> dict:\n",
    "    \"\"\"\n",
    "    Reads and analyzes multiple Parquet files located in the specified directory.\n",
    "\n",
    "    Parameters:\n",
    "    data_dir (str): The directory where the Parquet files are located.\n",
    "    file_names (list): A list of Parquet file names to read and analyze.\n",
    "    logger (logging.Logger): The logger instance.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary containing the file names as keys and their corresponding Dask DataFrames as values.\n",
    "    \"\"\"\n",
    "    dict_df = {}\n",
    "    \n",
    "    for file_name in file_names:\n",
    "        file_path = os.path.join(data_dir, file_name)\n",
    "        dask_df = read_parquet_file(file_path, logger)\n",
    "        \n",
    "        if dask_df is not None:\n",
    "            row_count = dask_df.shape[0].compute()\n",
    "            if row_count == 0:\n",
    "                error_message = f\"File {file_name} is empty.\"\n",
    "                print_error(logger, error_message)\n",
    "                raise ValueError(error_message)\n",
    "            else:\n",
    "                analyze_dask_dataframe(dask_df, file_name, logger)\n",
    "                dict_df.update({file_name: dask_df})\n",
    "        else:\n",
    "            error_message = f\"Failed to load {file_name}. No DataFrame returned.\"\n",
    "            print_error(logger, error_message)\n",
    "    \n",
    "    return dict_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "logger=None\n",
    "logger = configure_logging(log_dir='log')\n",
    "\n",
    "data_directory = os.path.join(os.getcwd(), 'data', 'raw')\n",
    "\n",
    "file_names = ['members.parquet', 'transactions.parquet', 'user_logs.parquet']\n",
    "custom_print(logger, '1.Read and Analyze Data', space_lines=True)\n",
    "dict_df = read_and_analyze_files(data_directory, file_names, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
